{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SME_WebScrapping.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjvAB9tcZx5b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "\n",
        "twitter_cred = dict()\n",
        "'''\n",
        "twitter_cred['CONSUMER_KEY'] = '***'\n",
        "twitter_cred['CONSUMER_SECRET'] = '****'\n",
        "twitter_cred['ACCESS_KEY'] = '***-***'\n",
        "twitter_cred['ACCESS_SECRET'] = '****'\n",
        "'''\n",
        "twitter_cred['CONSUMER_KEY'] = 'pDHudtJoB0opb2U9JoZr6ge2Y'\n",
        "twitter_cred['CONSUMER_SECRET'] = 'Z39VH8rx45WAlGjSFuuonmjZclo9kWsHPPJs46IobujPbQr5WW'\n",
        "twitter_cred['ACCESS_KEY'] = '151774740-ahQRcASlsrfDOFAhIYoSb9Kc0q7mWWR3RJ1o1Czq'\n",
        "twitter_cred['ACCESS_SECRET'] = 'koLzEjvqSZMOfVxEveIZ3VhSHmnGPFaPzAcOU8bzwuhnY'\n",
        "\n",
        "with open('twitter_credentials.json', 'w') as secret_info:\n",
        "    json.dump(twitter_cred, secret_info, indent=4, sort_keys=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qVjFL_VJu2lQ",
        "colab": {}
      },
      "source": [
        "import tweepy\n",
        "import csv\n",
        "import json\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import re,string\n",
        "\n",
        "with open('twitter_credentials.json') as cred_data:\n",
        "  info = json.load(cred_data)\n",
        "  consumer_key = info['CONSUMER_KEY']\n",
        "  consumer_secret = info['CONSUMER_SECRET']\n",
        "  access_key = info['ACCESS_KEY']\n",
        "  access_secret = info['ACCESS_SECRET']\n",
        "  \n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_key, access_secret)\n",
        "api = tweepy.API(auth)\n",
        "  \n",
        "  \n",
        "all_the_tweets = []\n",
        "final_op = []\n",
        "#keywords = [\"product\",\"launch\",\"sales\",\"release\",\"revenue\",\"profit\",\"loss\",\"expectation\",\"market\",\"growth\",\"country\",\"satisfaction\",\"satisfied\",\"happy\",\"great\",\"superb\",\"worst\",\"bad\",\"good\",\"unacceptable\",\"apology\",\"sorry\"]\n",
        "account_list = ['@SunflowerLab','@SyberryCorp','@iflexion','@indianic','@BelatrixSF','@zcocorporation','@algoworks','@EchoLogistics','@Softura','@DogtownMedia','@SumatoSoft','@openxcell','@openGeekslab','@DockYard','@promatics','@ZealouSys','@xtreemsolution','@jetrubyagency','@corelley_','@omsoftware','@konstantinfo','@tablexi','@SimbirSoft_Ltd','@tkxel','@endivesoftware','@consagous','@nexsoftsys','@octalitsolution','@Terasol_app','@KitelyTech','@Clavax','@iqlance','@HQSoftware','@mobcoderllc','@brain_mobi']\n",
        "#account_list = ['@indianic']\n",
        "\n",
        "startDate = datetime.datetime(2018, 9, 18, 0, 0, 0)\n",
        "endDate =   datetime.datetime(2019, 9, 17, 0, 0, 0)\n",
        "\n",
        "\n",
        "if len(account_list) > 0:  \n",
        "  for target in account_list:\n",
        "    #print(\"Getting data for \" + target)\n",
        "    #item = auth_api.get_user(target)\n",
        "    new_tweets = api.user_timeline(screen_name=target, count=23, include_rts = False, tweet_mode = 'extended', exclude_replies=True)  \n",
        "    # saving the most recent tweets\n",
        "    all_the_tweets.extend(new_tweets)\n",
        "    \n",
        "    '''for tweet in all_the_tweets:\n",
        "      outtweets=[]\n",
        "      #print(tweet)\n",
        "      if any(keyword in str(tweet.full_text.lower()) for keyword in keywords):\n",
        "        outtweets = [tweet.id_str, tweet.user.name, tweet.created_at, str(tweet.full_text)]\n",
        "        final_op.append(outtweets)\n",
        "        #print(outtweets)  \n",
        "     '''   \n",
        "    \n",
        "    for tweet in all_the_tweets:\n",
        "      outtweets=[]\n",
        "      if tweet.created_at < endDate and tweet.created_at > startDate:\n",
        "        outtweets = [tweet.id_str, tweet.user.name, tweet.created_at,tweet.user.location, str(tweet.full_text),tweet.user.description]\n",
        "        final_op.append(outtweets)\n",
        "        #print(tweet)  \n",
        "        #print(outtweets)\n",
        "        \n",
        "        \n",
        "    '''  \n",
        "    for tweet in all_the_tweets:\n",
        "      outtweets=[]\n",
        "      outtweets = [tweet.id_str, tweet.user.name, tweet.created_at,tweet.user.location, str(tweet.full_text),tweet.user.description]\n",
        "      final_op.append(outtweets)\n",
        "      #print(tweet)  \n",
        "      print(outtweets)  \n",
        "    '''\n",
        "#print(final_op)          \n",
        "df = pd.DataFrame(final_op)\n",
        "#df = df.transpose()\n",
        "df.columns = [\"Id\", \"SME\", \"Tweet_Date\",\"Location\", \"Tweets\",\"Company_Description\"] \n",
        "df.drop_duplicates(subset =\"Id\",inplace=True)          \n",
        "#df.head()        \n",
        "\n",
        "\n",
        "def text_separated(dataframe, columnname, keyword):\n",
        "    twt_txts = []\n",
        "    for twts in dataframe[columnname]: \n",
        "        twt_txts.append(twts)\n",
        "        \n",
        "    twts_cleaned = []\n",
        "    for twts_2 in twt_txts:\n",
        "        cleaned_twt = twts_2[:twts_2.find(keyword)-1] \n",
        "        twts_cleaned.append(cleaned_twt)\n",
        "    return(twts_cleaned)\n",
        "  \n",
        "  \n",
        "def strip_all_entities(text, removecharacter):\n",
        "    twt_txts = []\n",
        "    for twts in text: \n",
        "        twt_txts.append(twts)\n",
        "    \n",
        "    twts_cleaned = []\n",
        "    for word in twt_txts:\n",
        "      #print(word)\n",
        "      #cleaned_twt = word[:twts_2.find(keyword)-1] \n",
        "      cleaned_twt = word.replace(removecharacter, \" \")\n",
        "      #print(cleaned_twt)\n",
        "      twts_cleaned.append(cleaned_twt)\n",
        "    return(twts_cleaned)\n",
        "  \n",
        "def deEmojify(inputString):\n",
        "  twt_txts = []\n",
        "  for twts in inputString: \n",
        "    twt_txts.append(twts)\n",
        "    twts_cleaned = []\n",
        "  for twts_2 in twt_txts:\n",
        "    cleaned_twt = twts_2.encode('ascii', 'ignore').decode('ascii')\n",
        "    twts_cleaned.append(cleaned_twt)\n",
        "  return twts_cleaned\n",
        "  \n",
        "df['Cleaned_Tweets'] = text_separated(df, 'Tweets', 'https')\n",
        "df['Cleaned_Tweets'] = strip_all_entities(df['Cleaned_Tweets'], \"#\")\n",
        "df['Cleaned_Tweets'] = strip_all_entities(df['Cleaned_Tweets'], \"@\")\n",
        "df['Cleaned_Tweets'] = deEmojify(df['Cleaned_Tweets'])\n",
        "df['Cleaned_Tweets'] = strip_all_entities(df['Cleaned_Tweets'], \"\\n\")\n",
        "\n",
        "export_excel = df.to_excel (r'sme_usa_tweets.xlsx', index = None, header=True)\n",
        "\n",
        "df.drop([\"Id\", \"Location\", \"Tweets\",\"Company_Description\"], axis = 1, inplace = True) \n",
        "export_excel = df.to_excel (r'sme_cleaned_tweets.xlsx', index = None, header=True)\n",
        "\n",
        "\n",
        "#df['Cleaned_Tweets']\n",
        "#export_excel = df.to_excel (r'sme_usa_tweets.xlsx', index = None, header=True)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JBnYEAHruor",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}